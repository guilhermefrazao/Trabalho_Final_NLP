ğŸ“˜ IntroduÃ§Ã£o

Modelos de linguagem pequenos (como DistilGPT, TinyLlama ou versÃµes reduzidas de LLaMA e Mistral) sÃ£o ideais para aplicaÃ§Ãµes locais, embarcadas ou que rodam em servidores com poucos recursos.
Mesmo assim, o uso de memÃ³ria pode ser um gargalo importante â€” especialmente durante a inferÃªncia e o treinamento fino (fine-tuning).

Este guia explica como reduzir o consumo de memÃ³ria e tornar seu modelo mais eficiente sem perder muita qualidade.

âš™ï¸ 1. Entendendo o Consumo de MemÃ³ria

O uso de memÃ³ria em um LLM vem de trÃªs fontes principais:

Pesos do modelo â€” os parÃ¢metros treinados (ex: 1B parÃ¢metros â‰ˆ 4 GB em float32).

AtivaÃ§Ãµes â€” valores temporÃ¡rios gerados durante a inferÃªncia ou o treinamento.

Buffers e gradientes â€” usados apenas durante o treinamento.

ğŸ”¹ Dica: durante a inferÃªncia, apenas os pesos e ativaÃ§Ãµes importam. JÃ¡ durante o fine-tuning, os gradientes dobram (ou triplicam) o uso de memÃ³ria.

ğŸ§© 2. QuantizaÃ§Ã£o

QuantizaÃ§Ã£o converte pesos de precisÃ£o alta (ex: float32) para formatos menores (int8, int4, fp16).

ğŸ”§ TÃ©cnicas comuns:
TÃ©cnica	DescriÃ§Ã£o	Ganho tÃ­pico
FP16	Usa meia precisÃ£o (metade dos bits).	~2Ã— menos memÃ³ria
INT8	Quantiza pesos inteiros com calibraÃ§Ã£o.	~4Ã— menos memÃ³ria
INT4	Extremamente compacta, pode perder precisÃ£o.	~8Ã— menos memÃ³ria

ğŸ“¦ Ferramentas Ãºteis:

bitsandbytes

transformers + accelerate

GGUF / GPTQ / AWQ quantization formats

ğŸ”„ 3. Offloading e Streaming

Quando a GPU nÃ£o comporta todo o modelo, Ã© possÃ­vel dividir o carregamento entre:

GPU + CPU (offloading parcial)

Disco + RAM (streaming de pesos sob demanda)

ğŸ“˜ Ferramentas:

accelerate (Hugging Face)

torch.device_map="auto" para divisÃ£o automÃ¡tica

llama.cpp e exllama â€” executam quantizados direto em CPU

ğŸ§  4. Poda de Pesos (Pruning)

Remove conexÃµes pouco importantes, tornando o modelo mais leve.

Tipos:

Unstructured pruning: remove pesos isolados.

Structured pruning: remove neurÃ´nios ou cabeÃ§as de atenÃ§Ã£o inteiras.

â¡ï¸ Ideal para quando se quer um modelo menor sem precisar reescrever a arquitetura.

ğŸ” 5. Checkpoint Sharding e Lazy Loading

Durante o carregamento do modelo:

Use lazy loading (carregar pesos apenas quando necessÃ¡rios).

Divida checkpoints grandes em partes menores (shards).

Exemplo com transformers:

from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
    "tinyllama/TinyLlama-1.1B",
    device_map="auto",
    low_cpu_mem_usage=True
)

ğŸ’¡ 6. Fine-Tuning Eficiente

Para treinar modelos pequenos com pouca memÃ³ria:

Use LoRA / QLoRA: apenas pequenas matrizes adicionais sÃ£o treinadas.

Aplique gradiente acumulado para usar lotes menores.

Desative gradientes desnecessÃ¡rios com torch.no_grad() durante inferÃªncia.

ğŸ” 7. Monitoramento e Profiling

Use ferramentas para medir o uso real de memÃ³ria:

import torch
print(torch.cuda.memory_allocated() / 1e6, "MB")


Ou:

torch.profiler

nvidia-smi

accelerate.memory_tracker

âœ… ConclusÃ£o

Mesmo modelos pequenos podem ser otimizados significativamente.
Com quantizaÃ§Ã£o, offloading e tÃ©cnicas como LoRA, Ã© possÃ­vel rodar LLMs em notebooks, servidores leves ou atÃ© dispositivos embarcados.

ğŸ’¬ â€œEficiÃªncia nÃ£o Ã© sÃ³ ter menos parÃ¢metros â€” Ã© saber onde cada byte faz diferenÃ§a.â€

Quer que eu adicione um exemplo prÃ¡tico (por exemplo, usando um modelo quantizado do Hugging Face rodando localmente)? Isso deixaria o README mais aplicado.